{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "      <th>transliterated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C45.688</td>\n",
       "      <td>Dada taratari</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>Dada taratari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C45.635.5</td>\n",
       "      <td>Tumi korbe Amar sathe</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>Tumi korbe Amar sathe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C47.103</td>\n",
       "      <td>Ar ta chara a sob bessha peter cheleder okhan ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Ar ta chara a sob bessha peter cheleder okhan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C68.147</td>\n",
       "      <td>কাকের শরীরে ময়ুরের পাখা লাগিয়েছে</td>\n",
       "      <td>CAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>kākera śarīre maয়urera pākhā lāgiয়eche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C45.615</td>\n",
       "      <td>পতিতাদের চরিত্র রাজনৈতিক নেতাদের থেকে হাজার গু...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>patitādera caritra rājanaitika netādera theke ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               Text Sub-task A  \\\n",
       "0    C45.688                                      Dada taratari        NAG   \n",
       "1  C45.635.5                              Tumi korbe Amar sathe        NAG   \n",
       "2    C47.103  Ar ta chara a sob bessha peter cheleder okhan ...        OAG   \n",
       "3    C68.147                   কাকের শরীরে ময়ুরের পাখা লাগিয়েছে        CAG   \n",
       "4    C45.615  পতিতাদের চরিত্র রাজনৈতিক নেতাদের থেকে হাজার গু...        NAG   \n",
       "\n",
       "  Sub-task B                                     transliterated  \n",
       "0       NGEN                                      Dada taratari  \n",
       "1       NGEN                              Tumi korbe Amar sathe  \n",
       "2        GEN  Ar ta chara a sob bessha peter cheleder okhan ...  \n",
       "3       NGEN             kākera śarīre maয়urera pākhā lāgiয়eche  \n",
       "4       NGEN  patitādera caritra rājanaitika netādera theke ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_source = './iben/iben/trac2_iben_train_transliterated.csv'\n",
    "test_data_source = './iben/iben/trac2_iben_dev_transliterated.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_source,)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "      <th>transliterated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C59.2078</td>\n",
       "      <td>Ek dom sothik kotha bolecho jhekane theka uthe...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>Ek dom sothik kotha bolecho jhekane theka uthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C68.537</td>\n",
       "      <td>ফালতু মেয়ে</td>\n",
       "      <td>CAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>phālatu meya়e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C59.1344</td>\n",
       "      <td>DARUN AKDOM THIK</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>DARUN AKDOM THIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C59.706</td>\n",
       "      <td>Sala ranu magi</td>\n",
       "      <td>OAG</td>\n",
       "      <td>GEN</td>\n",
       "      <td>Sala ranu magi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C68.663</td>\n",
       "      <td>থামবেল ঠিককোরে বানা</td>\n",
       "      <td>OAG</td>\n",
       "      <td>NGEN</td>\n",
       "      <td>thāmavela ṭhikakore vānā</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text Sub-task A  \\\n",
       "0  C59.2078  Ek dom sothik kotha bolecho jhekane theka uthe...        NAG   \n",
       "1   C68.537                                        ফালতু মেয়ে        CAG   \n",
       "2  C59.1344                                   DARUN AKDOM THIK        NAG   \n",
       "3   C59.706                                     Sala ranu magi        OAG   \n",
       "4   C68.663                                থামবেল ঠিককোরে বানা        OAG   \n",
       "\n",
       "  Sub-task B                                     transliterated  \n",
       "0       NGEN  Ek dom sothik kotha bolecho jhekane theka uthe...  \n",
       "1       NGEN                                     phālatu meya়e  \n",
       "2       NGEN                                   DARUN AKDOM THIK  \n",
       "3        GEN                                     Sala ranu magi  \n",
       "4       NGEN                           thāmavela ṭhikakore vānā  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_source)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to lower case\n",
    "train_texts = train_df['Text'].values\n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "\n",
    "test_texts = test_df['Text'].values\n",
    "test_texts = [s.lower() for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3826.000000\n",
       "mean       50.225301\n",
       "std        63.396208\n",
       "min         3.000000\n",
       "25%        17.000000\n",
       "50%        31.000000\n",
       "75%        58.000000\n",
       "max       840.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['length'] = list(map(lambda x: len(x), train_df['Text']))\n",
    "train_df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05279665446941976"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df['length']> 150])/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGEN    3114\n",
       "GEN      712\n",
       "Name: Sub-task B, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = 'Sub-task B'\n",
    "train_df[task].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================Convert string to index================\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "# -----------------------Skip part end----------------------------\n",
    "\n",
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=150, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=150, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')\n",
    "\n",
    "# =======================Get classes================\n",
    "train_df[task]= pd.Categorical(train_df[task])\n",
    "train_df['target_class'] = train_df[task].cat.codes\n",
    "#train_class_list = [x - 1 for x in train_classes]\n",
    "\n",
    "test_df[task] = pd.Categorical(test_df[task])\n",
    "test_df['target_class'] = test_df[task].cat.codes\n",
    "#test_class_list = [x - 1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "#Y = pd.get_dummies(train_data['Sub-task A']).values\n",
    "train_classes = to_categorical(train_df['target_class'])\n",
    "test_classes = to_categorical(test_df['target_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    766\n",
       "0    191\n",
       "Name: target_class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['target_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 150\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 100\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = len(train_df['target_class'].value_counts())\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 150, 100)          7000      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 144, 256)          179456    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 144, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 48, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 42, 256)           459008    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 42, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 12, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 10, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 6, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 3,009,882\n",
      "Trainable params: 3,009,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Embedding weights\n",
    "embedding_weights = []  # (70, 69)\n",
    "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
    "\n",
    "for char, i in tk.word_index.items():  # from index 1 to 69\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i - 1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print('Load')\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            #weights=[embedding_weights]\n",
    "                           )\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
    "model.summary()\n",
    "\n",
    "# Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices]\n",
    "y_train = train_classes[indices]\n",
    "\n",
    "x_test = test_data\n",
    "y_test = test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 30\n",
    "model_file = 'ben_trans_' + task + '.h5'\n",
    "max_f1 = 0\n",
    "if os.path.exists(model_file):\n",
    "    model = load_model(model_file)\n",
    "    y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)    \n",
    "    print(classification_report(test_df['target_class'], y_pred_bool))\n",
    "    max_f1 = f1_score(test_df['target_class'], y_pred_bool, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 2s 525us/step - loss: 0.5026 - acc: 0.8095 - val_loss: 0.5031 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 205us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "saved t epoch  0  with f1  0.7116891087511698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 243us/step - loss: 0.4849 - acc: 0.8139 - val_loss: 0.5032 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "2\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 243us/step - loss: 0.4845 - acc: 0.8139 - val_loss: 0.5068 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "3\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.4741 - acc: 0.8139 - val_loss: 0.4881 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "4\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.4575 - acc: 0.8139 - val_loss: 0.4905 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "5\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.4350 - acc: 0.8139 - val_loss: 0.5506 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "6\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.4300 - acc: 0.8139 - val_loss: 0.4909 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "7\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.4294 - acc: 0.8139 - val_loss: 0.4965 - val_acc: 0.8004\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.80      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       957\n",
      "   macro avg       0.40      0.50      0.44       957\n",
      "weighted avg       0.64      0.80      0.71       957\n",
      "\n",
      "8\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.3924 - acc: 0.8129 - val_loss: 0.5768 - val_acc: 0.8067\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.04      0.08       191\n",
      "           1       0.81      1.00      0.89       766\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       957\n",
      "   macro avg       0.80      0.52      0.49       957\n",
      "weighted avg       0.81      0.81      0.73       957\n",
      "\n",
      "saved t epoch  8  with f1  0.7298618273504094\n",
      "9\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.3542 - acc: 0.8474 - val_loss: 0.6113 - val_acc: 0.8245\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.14      0.24       191\n",
      "           1       0.82      1.00      0.90       766\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       957\n",
      "   macro avg       0.86      0.57      0.57       957\n",
      "weighted avg       0.84      0.82      0.77       957\n",
      "\n",
      "saved t epoch  9  with f1  0.7682116116997849\n",
      "10\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 246us/step - loss: 0.3058 - acc: 0.8706 - val_loss: 0.5830 - val_acc: 0.8359\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.31      0.43       191\n",
      "           1       0.85      0.97      0.90       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.78      0.64      0.67       957\n",
      "weighted avg       0.82      0.84      0.81       957\n",
      "\n",
      "saved t epoch  10  with f1  0.8093846759021066\n",
      "11\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.2541 - acc: 0.8962 - val_loss: 0.6577 - val_acc: 0.8370\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.23      0.36       191\n",
      "           1       0.84      0.99      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.84      0.61      0.63       957\n",
      "weighted avg       0.84      0.84      0.80       957\n",
      "\n",
      "12\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.2725 - acc: 0.8936 - val_loss: 0.4635 - val_acc: 0.8380\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.28      0.41       191\n",
      "           1       0.84      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.80      0.63      0.66       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n",
      "13\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.2459 - acc: 0.9004 - val_loss: 0.7118 - val_acc: 0.8433\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.25      0.39       191\n",
      "           1       0.84      0.99      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.86      0.62      0.65       957\n",
      "weighted avg       0.85      0.84      0.81       957\n",
      "\n",
      "14\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.2119 - acc: 0.9151 - val_loss: 0.7427 - val_acc: 0.8433\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.26      0.40       191\n",
      "           1       0.84      0.99      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.85      0.63      0.65       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "15\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.1950 - acc: 0.9208 - val_loss: 0.8078 - val_acc: 0.8443\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.28      0.42       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.83      0.63      0.67       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "saved t epoch  15  with f1  0.8123141985368203\n",
      "16\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.2103 - acc: 0.9195 - val_loss: 0.7165 - val_acc: 0.8412\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.27      0.40       191\n",
      "           1       0.84      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.83      0.63      0.66       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "17\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1881 - acc: 0.9224 - val_loss: 0.8674 - val_acc: 0.8328\n",
      "957/957 [==============================] - 0s 77us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.28      0.40       191\n",
      "           1       0.84      0.97      0.90       766\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       957\n",
      "   macro avg       0.78      0.62      0.65       957\n",
      "weighted avg       0.82      0.83      0.80       957\n",
      "\n",
      "18\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1783 - acc: 0.9260 - val_loss: 0.9369 - val_acc: 0.8339\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.27      0.39       191\n",
      "           1       0.84      0.98      0.90       766\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       957\n",
      "   macro avg       0.79      0.62      0.65       957\n",
      "weighted avg       0.82      0.83      0.80       957\n",
      "\n",
      "19\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 244us/step - loss: 0.1821 - acc: 0.9258 - val_loss: 0.9191 - val_acc: 0.8245\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.34      0.43       191\n",
      "           1       0.85      0.95      0.90       766\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       957\n",
      "   macro avg       0.73      0.64      0.66       957\n",
      "weighted avg       0.80      0.82      0.80       957\n",
      "\n",
      "20\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.2067 - acc: 0.9171 - val_loss: 0.8243 - val_acc: 0.8443\n",
      "957/957 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.28      0.42       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.83      0.63      0.67       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "21\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.2069 - acc: 0.9192 - val_loss: 0.6853 - val_acc: 0.8318\n",
      "957/957 [==============================] - 0s 77us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.31      0.43       191\n",
      "           1       0.85      0.96      0.90       766\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       957\n",
      "   macro avg       0.76      0.64      0.66       957\n",
      "weighted avg       0.81      0.83      0.81       957\n",
      "\n",
      "22\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1883 - acc: 0.9239 - val_loss: 0.7752 - val_acc: 0.8412\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.29      0.42       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.81      0.63      0.66       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n",
      "23\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 246us/step - loss: 0.1753 - acc: 0.9281 - val_loss: 1.0302 - val_acc: 0.8422\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.28      0.41       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.82      0.63      0.66       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "24\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1761 - acc: 0.9279 - val_loss: 0.9189 - val_acc: 0.8328\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.25      0.37       191\n",
      "           1       0.84      0.98      0.90       766\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       957\n",
      "   macro avg       0.79      0.61      0.64       957\n",
      "weighted avg       0.82      0.83      0.80       957\n",
      "\n",
      "25\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 246us/step - loss: 0.1723 - acc: 0.9286 - val_loss: 1.0290 - val_acc: 0.8391\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.29      0.42       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.80      0.63      0.66       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n",
      "26\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 246us/step - loss: 0.1720 - acc: 0.9284 - val_loss: 1.1102 - val_acc: 0.8443\n",
      "957/957 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.28      0.42       191\n",
      "           1       0.85      0.99      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.84      0.63      0.66       957\n",
      "weighted avg       0.84      0.84      0.81       957\n",
      "\n",
      "27\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1903 - acc: 0.9247 - val_loss: 0.7829 - val_acc: 0.8401\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.27      0.40       191\n",
      "           1       0.84      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.82      0.63      0.66       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n",
      "28\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 246us/step - loss: 0.1964 - acc: 0.9208 - val_loss: 0.8531 - val_acc: 0.8359\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.30      0.42       191\n",
      "           1       0.85      0.97      0.90       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.78      0.64      0.66       957\n",
      "weighted avg       0.82      0.84      0.81       957\n",
      "\n",
      "29\n",
      "Train on 3826 samples, validate on 957 samples\n",
      "Epoch 1/1\n",
      "3826/3826 [==============================] - 1s 245us/step - loss: 0.1757 - acc: 0.9276 - val_loss: 0.8966 - val_acc: 0.8422\n",
      "957/957 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.30      0.43       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.81      0.64      0.67       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n",
      "saved t epoch  29  with f1  0.8137439702618615\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print(i)\n",
    "    model.fit(x_train, y_train,\n",
    "              validation_data=(x_test, y_test),\n",
    "              batch_size=128,\n",
    "              epochs=1,\n",
    "              verbose=1)\n",
    "    y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)    \n",
    "    print(classification_report(test_df['target_class'], y_pred_bool))\n",
    "    f1s = classification_report(test_df['target_class'], y_pred_bool, output_dict=True)['weighted avg']['f1-score']\n",
    "    if f1s > max_f1:\n",
    "        print('saved t epoch ', i, ' with f1 ', f1s)\n",
    "        model.save(model_file)\n",
    "        max_f1 = f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957/957 [==============================] - 0s 93us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.30      0.43       191\n",
      "           1       0.85      0.98      0.91       766\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       957\n",
      "   macro avg       0.81      0.64      0.67       957\n",
      "weighted avg       0.83      0.84      0.81       957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(test_df['target_class'], y_pred_bool))\n",
    "#print(classification_report(test_df['target_class'], y_pred_bool, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
