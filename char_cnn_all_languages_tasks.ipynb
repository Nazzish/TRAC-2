{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C45.451</td>\n",
       "      <td>Next part</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C47.11</td>\n",
       "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C33.79</td>\n",
       "      <td>ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4.1961</td>\n",
       "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C10.153</td>\n",
       "      <td>Concerned authorities should bring arundathi R...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               Text Sub-task A  \\\n",
       "0  C45.451                                          Next part        NAG   \n",
       "1   C47.11                 Iii8mllllllm\\nMdxfvb8o90lplppi0005        NAG   \n",
       "2   C33.79  ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...        NAG   \n",
       "3  C4.1961  What the fuck was this? I respect shwetabh and...        NAG   \n",
       "4  C10.153  Concerned authorities should bring arundathi R...        NAG   \n",
       "\n",
       "  Sub-task B  \n",
       "0       NGEN  \n",
       "1       NGEN  \n",
       "2       NGEN  \n",
       "3       NGEN  \n",
       "4       NGEN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = 'eng'\n",
    "task = 'Sub-task B'\n",
    "if lang == 'ben':\n",
    "    train_data_source = './iben/iben/trac2_iben_train_transliterated.csv'\n",
    "    test_data_source = './iben/iben/trac2_iben_dev_transliterated.csv'\n",
    "elif lang == 'hin':\n",
    "    train_data_source = './hin/hin/trac2_hin_train_transliterated.csv'\n",
    "    test_data_source = './hin/hin/trac2_hin_dev_transliterated.csv'\n",
    "elif lang == 'eng':\n",
    "    train_data_source = './eng/eng/trac2_eng_train.csv'\n",
    "    test_data_source = './eng/eng/trac2_eng_dev.csv'\n",
    "train_df = pd.read_csv(train_data_source,)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sub-task A</th>\n",
       "      <th>Sub-task B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C7.2589</td>\n",
       "      <td>U deserve more subscribers. U really great.</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C68.872</td>\n",
       "      <td>Nice video....</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C36.762</td>\n",
       "      <td>sorry if i bother somebody.. iam a defence asp...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4.1540.1</td>\n",
       "      <td>Joker was amazing....it was not glamorised !.....</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C59.68</td>\n",
       "      <td>Nice baro</td>\n",
       "      <td>NAG</td>\n",
       "      <td>NGEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               Text Sub-task A  \\\n",
       "0    C7.2589        U deserve more subscribers. U really great.        NAG   \n",
       "1    C68.872                                     Nice video....        NAG   \n",
       "2    C36.762  sorry if i bother somebody.. iam a defence asp...        NAG   \n",
       "3  C4.1540.1  Joker was amazing....it was not glamorised !.....        NAG   \n",
       "4     C59.68                                          Nice baro        NAG   \n",
       "\n",
       "  Sub-task B  \n",
       "0       NGEN  \n",
       "1       NGEN  \n",
       "2        GEN  \n",
       "3       NGEN  \n",
       "4       NGEN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_source)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to lower case\n",
    "if lang == 'eng':\n",
    "    train_texts = train_df['Text'].values \n",
    "    test_texts = test_df['Text'].values\n",
    "else:\n",
    "    train_texts = train_df['transliterated'].values\n",
    "    test_texts = test_df['transliterated'].values\n",
    "\n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "test_texts = [s.lower() for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4263.000000\n",
       "mean       97.965752\n",
       "std       187.789251\n",
       "min         3.000000\n",
       "25%        20.000000\n",
       "50%        44.000000\n",
       "75%       104.000000\n",
       "max      4377.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['length'] = list(map(lambda x: len(x), train_df['Text']))\n",
    "train_df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1653764954257565"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df['length']> 150])/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGEN    3954\n",
       "GEN      309\n",
       "Name: Sub-task B, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[task].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =======================Convert string to index================\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "# -----------------------Skip part end----------------------------\n",
    "\n",
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=150, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=150, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')\n",
    "\n",
    "# =======================Get classes================\n",
    "train_df[task]= pd.Categorical(train_df[task])\n",
    "train_df['target_class'] = train_df[task].cat.codes\n",
    "#train_class_list = [x - 1 for x in train_classes]\n",
    "\n",
    "test_df[task] = pd.Categorical(test_df[task])\n",
    "test_df['target_class'] = test_df[task].cat.codes\n",
    "#test_class_list = [x - 1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "#Y = pd.get_dummies(train_data['Sub-task A']).values\n",
    "train_classes = to_categorical(train_df['target_class'])\n",
    "test_classes = to_categorical(test_df['target_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    993\n",
       "0     73\n",
       "Name: target_class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['target_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 150\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 100\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = len(train_df['target_class'].value_counts())\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 100)          7000      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 144, 256)          179456    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 144, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 42, 256)           459008    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 42, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 12, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 10, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 6, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 3,009,882\n",
      "Trainable params: 3,009,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Embedding weights\n",
    "embedding_weights = []  # (70, 69)\n",
    "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
    "\n",
    "for char, i in tk.word_index.items():  # from index 1 to 69\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i - 1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print('Load')\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            #weights=[embedding_weights]\n",
    "                           )\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
    "model.summary()\n",
    "\n",
    "# Shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices]\n",
    "y_train = train_classes[indices]\n",
    "\n",
    "x_test = test_data\n",
    "y_test = test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "epochs = 30\n",
    "model_file = lang +'_trans_' + task + '.h5'\n",
    "max_f1 = 0\n",
    "if os.path.exists(model_file):\n",
    "    model = load_model(model_file)\n",
    "    y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)    \n",
    "    print(classification_report(test_df['target_class'], y_pred_bool))\n",
    "    max_f1 = f1_score(test_df['target_class'], y_pred_bool, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.3021 - acc: 0.9120 - val_loss: 0.2496 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 284us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "saved at epoch  1  with f1  0.8984935035587139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.2571 - acc: 0.9275 - val_loss: 0.2400 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "2\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.2533 - acc: 0.9275 - val_loss: 0.2405 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "3\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.2516 - acc: 0.9275 - val_loss: 0.2461 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "4\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 244us/step - loss: 0.2435 - acc: 0.9275 - val_loss: 0.2796 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 81us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "5\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 245us/step - loss: 0.2454 - acc: 0.9278 - val_loss: 0.2440 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "6\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.2147 - acc: 0.9313 - val_loss: 0.2431 - val_acc: 0.9315\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "7\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.2066 - acc: 0.9364 - val_loss: 0.2594 - val_acc: 0.9090\n",
      "1066/1066 [==============================] - 0s 81us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.16      0.20        73\n",
      "           1       0.94      0.96      0.95       993\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1066\n",
      "   macro avg       0.60      0.56      0.58      1066\n",
      "weighted avg       0.89      0.91      0.90      1066\n",
      "\n",
      "saved at epoch  8  with f1  0.9001709875297603\n",
      "8\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.1760 - acc: 0.9472 - val_loss: 0.2891 - val_acc: 0.9325\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.03        73\n",
      "           1       0.93      1.00      0.97       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.97      0.51      0.50      1066\n",
      "weighted avg       0.94      0.93      0.90      1066\n",
      "\n",
      "saved at epoch  9  with f1  0.9007809082473095\n",
      "9\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.1645 - acc: 0.9554 - val_loss: 0.2488 - val_acc: 0.9287\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.07      0.12        73\n",
      "           1       0.94      0.99      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.66      0.53      0.54      1066\n",
      "weighted avg       0.90      0.93      0.90      1066\n",
      "\n",
      "saved at epoch  10  with f1  0.9048806206327294\n",
      "10\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.1334 - acc: 0.9655 - val_loss: 0.3420 - val_acc: 0.9306\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.93      1.00      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.47      0.50      0.48      1066\n",
      "weighted avg       0.87      0.93      0.90      1066\n",
      "\n",
      "11\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.1430 - acc: 0.9580 - val_loss: 0.2366 - val_acc: 0.9306\n",
      "1066/1066 [==============================] - 0s 78us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.15      0.23        73\n",
      "           1       0.94      0.99      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.71      0.57      0.60      1066\n",
      "weighted avg       0.91      0.93      0.91      1066\n",
      "\n",
      "saved at epoch  12  with f1  0.9133562955604619\n",
      "12\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0982 - acc: 0.9719 - val_loss: 0.3076 - val_acc: 0.9118\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.29      0.31        73\n",
      "           1       0.95      0.96      0.95       993\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1066\n",
      "   macro avg       0.64      0.62      0.63      1066\n",
      "weighted avg       0.91      0.91      0.91      1066\n",
      "\n",
      "13\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0633 - acc: 0.9803 - val_loss: 0.5957 - val_acc: 0.9353\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.10        73\n",
      "           1       0.94      1.00      0.97       993\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1066\n",
      "   macro avg       0.97      0.53      0.54      1066\n",
      "weighted avg       0.94      0.94      0.91      1066\n",
      "\n",
      "14\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0729 - acc: 0.9794 - val_loss: 0.5638 - val_acc: 0.9334\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.07      0.12        73\n",
      "           1       0.94      1.00      0.97       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.78      0.53      0.54      1066\n",
      "weighted avg       0.91      0.93      0.91      1066\n",
      "\n",
      "15\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0420 - acc: 0.9894 - val_loss: 0.4728 - val_acc: 0.9221\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24        73\n",
      "           1       0.94      0.98      0.96       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.65      0.58      0.60      1066\n",
      "weighted avg       0.90      0.92      0.91      1066\n",
      "\n",
      "16\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0344 - acc: 0.9909 - val_loss: 0.4644 - val_acc: 0.9156\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38        73\n",
      "           1       0.95      0.95      0.95       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.67      0.67      0.67      1066\n",
      "weighted avg       0.92      0.92      0.92      1066\n",
      "\n",
      "saved at epoch  17  with f1  0.9155722326454033\n",
      "17\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0349 - acc: 0.9916 - val_loss: 0.3752 - val_acc: 0.9296\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.10      0.16        73\n",
      "           1       0.94      0.99      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.69      0.54      0.56      1066\n",
      "weighted avg       0.90      0.93      0.91      1066\n",
      "\n",
      "18\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 248us/step - loss: 0.0180 - acc: 0.9955 - val_loss: 0.4927 - val_acc: 0.9390\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.22      0.33        73\n",
      "           1       0.95      0.99      0.97       993\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1066\n",
      "   macro avg       0.81      0.61      0.65      1066\n",
      "weighted avg       0.93      0.94      0.92      1066\n",
      "\n",
      "saved at epoch  19  with f1  0.924357438564127\n",
      "19\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0120 - acc: 0.9981 - val_loss: 0.6505 - val_acc: 0.9362\n",
      "1066/1066 [==============================] - 0s 79us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.27      0.37        73\n",
      "           1       0.95      0.98      0.97       993\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1066\n",
      "   macro avg       0.76      0.63      0.67      1066\n",
      "weighted avg       0.92      0.94      0.93      1066\n",
      "\n",
      "saved at epoch  20  with f1  0.925586657559086\n",
      "20\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 248us/step - loss: 0.0177 - acc: 0.9962 - val_loss: 0.4637 - val_acc: 0.9099\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.34      0.34        73\n",
      "           1       0.95      0.95      0.95       993\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1066\n",
      "   macro avg       0.65      0.65      0.65      1066\n",
      "weighted avg       0.91      0.91      0.91      1066\n",
      "\n",
      "21\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0280 - acc: 0.9930 - val_loss: 0.4002 - val_acc: 0.9231\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.37      0.40        73\n",
      "           1       0.95      0.96      0.96       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.69      0.67      0.68      1066\n",
      "weighted avg       0.92      0.92      0.92      1066\n",
      "\n",
      "22\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0274 - acc: 0.9923 - val_loss: 0.4900 - val_acc: 0.9343\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.26      0.35        73\n",
      "           1       0.95      0.98      0.97       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.75      0.62      0.66      1066\n",
      "weighted avg       0.92      0.93      0.92      1066\n",
      "\n",
      "23\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0267 - acc: 0.9946 - val_loss: 0.5022 - val_acc: 0.9325\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.23      0.32        73\n",
      "           1       0.95      0.98      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.73      0.61      0.64      1066\n",
      "weighted avg       0.92      0.93      0.92      1066\n",
      "\n",
      "24\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.0277 - acc: 0.9946 - val_loss: 0.3617 - val_acc: 0.9278\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.27      0.34        73\n",
      "           1       0.95      0.98      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.70      0.62      0.65      1066\n",
      "weighted avg       0.91      0.93      0.92      1066\n",
      "\n",
      "25\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0562 - acc: 0.9866 - val_loss: 0.4607 - val_acc: 0.9278\n",
      "1066/1066 [==============================] - 0s 81us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.18      0.25        73\n",
      "           1       0.94      0.98      0.96       993\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1066\n",
      "   macro avg       0.69      0.58      0.61      1066\n",
      "weighted avg       0.91      0.93      0.91      1066\n",
      "\n",
      "26\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0240 - acc: 0.9951 - val_loss: 0.5735 - val_acc: 0.9156\n",
      "1066/1066 [==============================] - 0s 81us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.32      0.34        73\n",
      "           1       0.95      0.96      0.95       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.66      0.64      0.65      1066\n",
      "weighted avg       0.91      0.92      0.91      1066\n",
      "\n",
      "27\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0161 - acc: 0.9965 - val_loss: 0.4845 - val_acc: 0.9203\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.26      0.31        73\n",
      "           1       0.95      0.97      0.96       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.66      0.61      0.63      1066\n",
      "weighted avg       0.91      0.92      0.91      1066\n",
      "\n",
      "28\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0109 - acc: 0.9981 - val_loss: 0.6126 - val_acc: 0.9193\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.27      0.32        73\n",
      "           1       0.95      0.97      0.96       993\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1066\n",
      "   macro avg       0.66      0.62      0.64      1066\n",
      "weighted avg       0.91      0.92      0.91      1066\n",
      "\n",
      "29\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.0403 - acc: 0.9897 - val_loss: 0.3586 - val_acc: 0.9371\n",
      "1066/1066 [==============================] - 0s 80us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.10      0.17        73\n",
      "           1       0.94      1.00      0.97       993\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1066\n",
      "   macro avg       0.91      0.55      0.57      1066\n",
      "weighted avg       0.93      0.94      0.91      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print(i)\n",
    "    model.fit(x_train, y_train,\n",
    "              validation_data=(x_test, y_test),\n",
    "              batch_size=128,\n",
    "              epochs=1,\n",
    "              verbose=1)\n",
    "    y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "    y_pred_bool = np.argmax(y_pred, axis=1)    \n",
    "    print(classification_report(test_df['target_class'], y_pred_bool))\n",
    "    f1s = classification_report(test_df['target_class'], y_pred_bool, output_dict=True)['weighted avg']['f1-score']\n",
    "    if f1s > max_f1:\n",
    "        print('saved at epoch ', i +1, ' with f1 ', f1s)\n",
    "        model.save(model_file)\n",
    "        max_f1 = f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 178us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.27      0.37        73\n",
      "           1       0.95      0.98      0.97       993\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1066\n",
      "   macro avg       0.76      0.63      0.67      1066\n",
      "weighted avg       0.92      0.94      0.93      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_file)\n",
    "y_pred = model.predict(x_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)    \n",
    "print(classification_report(test_df['target_class'], y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hin task a 66 task b 88\n",
    "#ben task a 69 task b 86\n",
    "#eng task a 75 task b 93"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
